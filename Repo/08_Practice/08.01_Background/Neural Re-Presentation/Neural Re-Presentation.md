![](media/neural%20representation.gif)

This work explored working with Reinforcement Learning agents, specifically multi-agent environments. In these environments inquiring through practice the relations these agents were able to make together and what where their capacities to learn and represent their environment. In this setup, which is one of a few I made, I oriented them to be able to communicate through a visual layer together whilst trying to navigate a simple task of not touching, but staying close to one another. The visual layer that they produced was their only mode of feedback to the environment and oriented them to form a visual language to communicate through, and one in which the viewer can intercept their communications.

This work was presented at 4S 2023, as part of their making and doing exhibition and received nice feedback from the community there. I did end up stopping to work with these agents for a few reasons, mainly because of some of the critiques I make in the [01.01.00_Background](../../../01_Introduction/01_entries/01.01.00_Background.md) of the introduction. These orient that RL algorithms centrally rely upon penal logics and outdated theories of animal and child learning to train these agents, and when I tried to find other ways of relating to them I couldn't. The only ways I could work with them was to execute their code, penalise them until they did what I wanted them to do, and constantly tried to cheat the rules or run away from me. When in search of more intimate computing it could not really be found here. Beyond this, they used a huge amount of energy to train and run the agents, and often with no guarantee of a good outcome. They felt like they were going to cook my computer, and not really for anything that generative.
