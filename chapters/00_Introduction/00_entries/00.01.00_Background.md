## Background

In a way this background to the research is so far away and so small on its metaphorical horizon that it only takes up this small section of the introduction. I still find it important to mention these now distant inquiries as it offers many of the reasons for why these Configure-able methods, my collaborative inquiries and this thesis have come into being. Coming into this PhD I was very much following the norms of computer science, and even though I had been tought to code creatively I had still been taught how to optimise code, automate decisions and of course train AI models to make them for me. With this sedimented norm in place, even though I was fairly set on collective knowledge creation, I of course started by making many different bits of code, databases, and critiques of how AI might do things with us or for us.

As I proceeded in this inherited practise and normalised path I started to read up and relate more to crip theory and its critiques of technology. One of these critiques that surfaced was of AI and computations histories and practices that originate from those of eugenics. Eugenics as a phony science believes that we can shape and cure the human race of unwanted and feeble minded individuals through genetic selection, and has a long history of violence toward disabled people and many other marginalised groups. Wendy Chun in here book *Discriminating Data* (2021) covers how these algorithms work on both mathematics and social levels through not only these eugenic logics, but also segregationist algorithms that split people into groups via different physical and social "features". Timnit Gebru and Émile Torres in their essay "The TESCREAL bundle" (2024), and as part of D.A.I.R.[^y6], also bring together current histories of how eugenic genetic curing have also been accelerated into silicon valley transhumanist imaginaries that cure humanity of our feeble and disposable bodies as a way to reach a "net good". To find this cure they are of course gambling everything on the yet out of reach Artificial Generalised Intelligence (AGI) and its unknown and often undeclared capabilities. Johannes Bruder in his chapter *AI as medium and message* (2023, 171) where he discuses how the imaginaries of AI both fetishise and want to cure Autistic and neurodiverse individuals in particular.

Later on in the flipping table (chap) I go on to talk more closely about crip takes on technology, and Alison Kafer's (Kafer 2013) crip critique of transhumanism, and this crip refusal of the imaginary and need to cure people through these ever unferling and never in reach speculative future technologies. In response Kafer offers a refusal of a future cure to instead to orient to what is in reach. In changing this focus, Kafer is asking how can communities form social and technologies politics and relations that can manifest the care and affirmative infrastructures now?

When inquiring through practise with AI, I developed a number of different speculative works to reflect through what was in reach of me with AI. These ranged from situated community made datasets that questioned the normalisation and isolation of indexes, as well as interactive bits of code that made accessible how AI algorithms segment space. The type of AI that I particularly oriented towards though and got to know through practice was that of Reinforcement Learning (RL). This was partially due to deepminds work with "mastering" Go and other closed system games (Silver et al. 2016; 2017), and as this made room to hype up to be the part of RL/AI that could encounter new things, the unknown, or be "creative". It was also because it could be used to train multi-agent environments and I wanted to explore how to form environments of collaboration and organisation within these terms. I went on to form works with RL, with the main one trying to configure these "agents" so that the group of them could communicate to one another through abstract patterns and emerge their own visual language and representation of their environment together. These came out okay, but I ended up desisting from working with them for a few reasons. The main one was after a while I realised that the logics of the RL algorithm that was supposed to enable AI to encounter the unknown, and within AGI imaginaries cure every yet unknown problem, was in simple a penal logic. This meant that the only feedback was to punish or reward the agent to reinforce it to form a "policy" for that environments task. It was also well noted to only work well in time sensitive environments if you keep giving a constant punishment to the agents at every step. In practice this system was very uncreative, the agent was mostly not understanding what you wanted it to do and always trying to trick the rules you laid out to just get the max points. When working in this relationship especially it I tried to figure out a relation and dynamic where I did not have to "master" or train them to get them to learn anything. Another reason is that I felt that a lot of this practice that I had done was very closed off and hard to collaborate with others through. Here I realised that for me to use AI critically and sustainably in any of my local community contexts was fairly far out of my reach at the time. None of this made this technology either accessible or desirable to me after a while. I slowly stopped to work with these AI agents in the end, as there seemed no way to collaborate with them without executing, mastering or punishing them, and the agents always seemed to want to run away or escape when we came into contact under these terms.

Another more direct explanation of this is again from D.A.I.R., but this time from an episode of the *Mystery AI Hype Theater 3000* podcast by linguist Prof. Emily M. Bender and sociologist Dr. Alex Hanna. In this podcast they often playfully take apart AI papers and their critical underpinning. I bring this podcast up though as I want to focus on Bender's critique of LLM's in the *Can Machines Learn To Behave? Part 3 (Hanna and Bender 2022)*. As she puts it[^y7], there are so many ways of modelling language that exist, and yet they try to fit it all through this one single algorithm that is based on eugenics, not overule any prior histories and practices of language or linguistics. In a similar orientation I realised that yes I might be able to organise with my community through AI and to fold up our issues into abstract eugenics logics to automate our decisions, but in doing this we lose the capacity and room to emerge the many other logics and algorithms that we can interdependently and locally manifest ourselves together from our divergent backgrounds. So as I disorient away from this background of a cured future through automation I ask how with the collaborations of this thesis I can make space for us to move beyond these normalising practises of computing that prescribed generalised logics of organisation onto our community? Through this thesis' research and its inquiries I felt out these Configure-Able methods as a way to form a response to this question and inquire into how communities can come together to configure out their social and technical infrastructures and politics locally.

Just to be clear this is not a total refusal of AI or to say that algorithms are defined by their origins. It is instead to acknowledge these histories and the limits of their logics when we do work with them. With this crip relation to technology, it is to be okay with technologies messiness, but to actively take care around/within their limits. For instance in my inquiries, it is to be okay with the AI speech to text algorithm that we have for now, but to both orient towards community driven and joyous live captions Like I did in my inquiries, but also to centres stenographer/transcription knowledges within automated captioning technologies like Louise Hickman's cripping of AI (*Louise Hickman* 2021). For Laura Forlano as well, is to be okay with the automated AI insulin pen (2023) and it's buggines, but in this to again centre points of impact and the person's embodied experience to these technologies in our critique of them.

[^y6]: https://www.dair-institute.org/
[^y7]: "Um but also then he goes on to say \"Real insight began to emerge with word2vec\" which is one of these first sort of neural approaches to language modeling. Um and it\'s like no all of the work in linguistics prior to 2013 that\'s looking at at the relationship between form and meaning, all the work on distributional semantics before then, none of that is real insight? Real insight is when the engineers come in and throw their mathy math at it? . . . . I don\'t think so. . . " (Hanna and Bender 2022)
